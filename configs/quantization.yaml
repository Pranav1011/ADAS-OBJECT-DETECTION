# Quantization Configuration

# Calibration
calibration:
  num_samples: 500       # Number of calibration images
  batch_size: 8          # Batch size for calibration

# ONNX Runtime Quantization
onnx:
  input_model: weights/best.onnx
  output_fp32: weights/best_fp32.onnx
  output_int8: weights/best_int8.onnx
  quant_format: QDQ      # QuantizeLinear/DequantizeLinear format
  per_channel: true      # Per-channel quantization for better accuracy
  activation_type: QUInt8
  weight_type: QInt8

# TensorRT Optimization (NVIDIA GPU only)
tensorrt:
  input_model: weights/best.onnx
  output_fp16: weights/best_fp16.engine
  output_int8: weights/best_int8.engine
  workspace_size: 1073741824  # 1GB workspace
  fp16: true
  int8: true
  dla_core: -1           # -1 = disabled, 0/1 for Jetson DLA

# CoreML Conversion (Apple Silicon only)
coreml:
  input_model: weights/best.pt
  output_fp16: weights/best_fp16.mlpackage
  compute_units: ALL     # ALL, CPU_ONLY, CPU_AND_GPU, CPU_AND_NE

# Benchmark settings
benchmark:
  warmup_runs: 50
  num_runs: 100
  input_size: [1, 3, 640, 640]  # NCHW format
