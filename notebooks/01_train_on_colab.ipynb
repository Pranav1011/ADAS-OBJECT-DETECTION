{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAS Object Detection - Training on Google Colab\n",
    "\n",
    "This notebook trains YOLOv8 on BDD100K dataset using Google Colab's free GPU.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload `bdd100k_images_100k.zip` to Google Drive at `My Drive/datasets/`\n",
    "2. Upload `bdd100k_labels_release.zip` to Google Drive at `My Drive/datasets/`\n",
    "\n",
    "**Runtime**: Select GPU runtime (Runtime > Change runtime type > GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Pranav1011/ADAS-OBJECT-DETECTION.git\n",
    "%cd ADAS-OBJECT-DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics opencv-python albumentations wandb pyyaml tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Copy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "!mkdir -p data/raw data/processed/images/train data/processed/images/val data/processed/labels/train data/processed/labels/val\n",
    "\n",
    "# Copy dataset from Drive\n",
    "print(\"Copying images (this may take a few minutes)...\")\n",
    "!cp \"/content/drive/MyDrive/datasets/bdd100k_images_100k.zip\" data/raw/\n",
    "print(\"Copying labels...\")\n",
    "!cp \"/content/drive/MyDrive/datasets/bdd100k_labels_release.zip\" data/raw/\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "print(\"Extracting images (this will take several minutes)...\")\n",
    "!cd data/raw && unzip -o -q bdd100k_images_100k.zip\n",
    "print(\"Extracting labels...\")\n",
    "!cd data/raw && unzip -o -q bdd100k_labels_release.zip\n",
    "print(\"Done!\")\n",
    "\n",
    "# Check extraction\n",
    "!ls -la data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert BDD100K to YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "CLASS_MAPPING = {\n",
    "    \"car\": 0, \"truck\": 1, \"bus\": 2, \"person\": 3,\n",
    "    \"rider\": 4, \"bike\": 5, \"motor\": 5,\n",
    "    \"traffic light\": 6, \"traffic sign\": 7\n",
    "}\n",
    "CLASS_NAMES = [\"car\", \"truck\", \"bus\", \"pedestrian\", \"cyclist\", \"motorcycle\", \"traffic_light\", \"traffic_sign\"]\n",
    "IMG_W, IMG_H = 1280, 720\n",
    "\n",
    "MAX_TRAIN = 8000  # Use 8k for training\n",
    "MAX_VAL = 2000    # Use 2k for validation\n",
    "\n",
    "def convert_box(box):\n",
    "    x1, y1, x2, y2 = box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]\n",
    "    cx = (x1 + x2) / 2 / IMG_W\n",
    "    cy = (y1 + y2) / 2 / IMG_H\n",
    "    w = (x2 - x1) / IMG_W\n",
    "    h = (y2 - y1) / IMG_H\n",
    "    return f\"{cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\"\n",
    "\n",
    "def process_split(img_dir, label_dir, out_img_dir, out_label_dir, max_images=None):\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images = list(img_dir.glob(\"*.jpg\"))\n",
    "    random.seed(42)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    if max_images:\n",
    "        images = images[:max_images * 2]  # Get more to account for no-label cases\n",
    "    \n",
    "    count = 0\n",
    "    class_counts = {name: 0 for name in CLASS_NAMES}\n",
    "    \n",
    "    for img_path in tqdm(images, desc=f\"Processing {img_dir.name}\"):\n",
    "        if max_images and count >= max_images:\n",
    "            break\n",
    "            \n",
    "        json_path = label_dir / f\"{img_path.stem}.json\"\n",
    "        if not json_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        lines = []\n",
    "        for frame in data.get(\"frames\", []):\n",
    "            for obj in frame.get(\"objects\", []):\n",
    "                cat = obj.get(\"category\", \"\").lower()\n",
    "                if cat not in CLASS_MAPPING:\n",
    "                    continue\n",
    "                if \"box2d\" not in obj:\n",
    "                    continue\n",
    "                class_id = CLASS_MAPPING[cat]\n",
    "                box_str = convert_box(obj[\"box2d\"])\n",
    "                lines.append(f\"{class_id} {box_str}\")\n",
    "                class_counts[CLASS_NAMES[class_id]] += 1\n",
    "        \n",
    "        if lines:\n",
    "            shutil.copy(img_path, out_img_dir / img_path.name)\n",
    "            with open(out_label_dir / f\"{img_path.stem}.txt\", \"w\") as f:\n",
    "                f.write(\"\\n\".join(lines))\n",
    "            count += 1\n",
    "    \n",
    "    return count, class_counts\n",
    "\n",
    "# Process train and val\n",
    "base = Path(\"data/raw\")\n",
    "out = Path(\"data/processed\")\n",
    "\n",
    "# Find the correct paths\n",
    "if (base / \"bdd100k/images/100k/train\").exists():\n",
    "    img_base = base / \"bdd100k/images/100k\"\n",
    "    label_base = base / \"bdd100k/labels/100k\"\n",
    "elif (base / \"100k/train\").exists():\n",
    "    img_base = base / \"100k\"\n",
    "    label_base = base / \"100k\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find BDD100K data. Check extraction.\")\n",
    "\n",
    "print(f\"Using images from: {img_base}\")\n",
    "print(f\"Using labels from: {label_base}\")\n",
    "print()\n",
    "\n",
    "train_count, train_classes = process_split(\n",
    "    img_base / \"train\", label_base / \"train\",\n",
    "    out / \"images/train\", out / \"labels/train\",\n",
    "    max_images=MAX_TRAIN\n",
    ")\n",
    "\n",
    "val_count, val_classes = process_split(\n",
    "    img_base / \"val\", label_base / \"val\",\n",
    "    out / \"images/val\", out / \"labels/val\",\n",
    "    max_images=MAX_VAL\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Train images: {train_count}\")\n",
    "print(f\"Val images: {val_count}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "for name, count in sorted(train_classes.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "\n",
    "# Generate dataset.yaml\n",
    "config = {\n",
    "    \"path\": str(out.absolute()),\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"names\": {i: n for i, n in enumerate(CLASS_NAMES)},\n",
    "    \"nc\": len(CLASS_NAMES)\n",
    "}\n",
    "with open(out / \"dataset.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f, sort_keys=False)\n",
    "    \n",
    "print(f\"\\nSaved dataset.yaml\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "!echo \"Train images:\" && ls data/processed/images/train | wc -l\n",
    "!echo \"Val images:\" && ls data/processed/images/val | wc -l\n",
    "!echo \"\\nDataset config:\"\n",
    "!cat data/processed/dataset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8m model (medium - good balance of speed/accuracy)\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data='data/processed/dataset.yaml',\n",
    "    epochs=100,\n",
    "    batch=16,           # Adjust: 8 for T4, 16-32 for A100\n",
    "    imgsz=640,\n",
    "    patience=20,        # Early stopping\n",
    "    device=0,\n",
    "    project='runs/train',\n",
    "    name='yolov8m-bdd100k',\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    warmup_epochs=5,\n",
    "    cos_lr=True,\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    translate=0.1,\n",
    "    scale=0.5,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "metrics = model.val(data='data/processed/dataset.yaml')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluation Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"mAP@0.5: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")\n",
    "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"Recall: {metrics.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights directory\n",
    "!mkdir -p weights\n",
    "\n",
    "# Copy best weights\n",
    "!cp runs/train/yolov8m-bdd100k/weights/best.pt weights/best.pt\n",
    "\n",
    "# Export to ONNX\n",
    "print(\"Exporting to ONNX...\")\n",
    "model = YOLO('weights/best.pt')\n",
    "model.export(format='onnx', imgsz=640, simplify=True, opset=12)\n",
    "!mv weights/best.onnx weights/best.onnx\n",
    "\n",
    "print(\"\\nExported models:\")\n",
    "!ls -lh weights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ONNX INT8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantFormat, QuantType\n",
    "\n",
    "class CalibrationDataLoader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_images, input_name=\"images\", img_size=640):\n",
    "        self.image_paths = calibration_images\n",
    "        self.input_name = input_name\n",
    "        self.img_size = img_size\n",
    "        self.index = 0\n",
    "\n",
    "    def preprocess(self, image_path):\n",
    "        img = cv2.imread(str(image_path))\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        return np.expand_dims(img, 0)\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.image_paths):\n",
    "            return None\n",
    "        image_path = self.image_paths[self.index]\n",
    "        self.index += 1\n",
    "        return {self.input_name: self.preprocess(image_path)}\n",
    "\n",
    "    def rewind(self):\n",
    "        self.index = 0\n",
    "\n",
    "# Get calibration images\n",
    "calibration_images = list(Path(\"data/processed/images/val\").glob(\"*.jpg\"))[:200]\n",
    "print(f\"Using {len(calibration_images)} images for calibration\")\n",
    "\n",
    "# Create calibration data reader\n",
    "calibration_reader = CalibrationDataLoader(calibration_images)\n",
    "\n",
    "# Quantize\n",
    "print(\"Quantizing to INT8 (this may take a few minutes)...\")\n",
    "quantize_static(\n",
    "    model_input=\"weights/best.onnx\",\n",
    "    model_output=\"weights/best_int8.onnx\",\n",
    "    calibration_data_reader=calibration_reader,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    per_channel=True,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(\"\\nQuantized models:\")\n",
    "!ls -lh weights/*.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorRT Export (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TensorRT FP16\n",
    "print(\"Exporting to TensorRT FP16...\")\n",
    "model = YOLO('weights/best.pt')\n",
    "model.export(format='engine', imgsz=640, half=True, device=0)\n",
    "\n",
    "!mv weights/best.engine weights/best_fp16.engine 2>/dev/null || true\n",
    "\n",
    "print(\"\\nAll exported models:\")\n",
    "!ls -lh weights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_onnx(model_path, num_runs=100):\n",
    "    session = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    dummy = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        session.run(None, {input_name: dummy})\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        session.run(None, {input_name: dummy})\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "def benchmark_pytorch(model_path, num_runs=100):\n",
    "    import torch\n",
    "    model = YOLO(model_path)\n",
    "    dummy = torch.randn(1, 3, 640, 640).cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        model.model(dummy)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        model.model(dummy)\n",
    "        torch.cuda.synchronize()\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<25} {'Size (MB)':<12} {'Latency (ms)':<15} {'FPS':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# PyTorch FP32\n",
    "size = Path('weights/best.pt').stat().st_size / 1024 / 1024\n",
    "mean, std = benchmark_pytorch('weights/best.pt')\n",
    "print(f\"{'PyTorch FP32':<25} {size:<12.1f} {mean:<15.2f} {1000/mean:<10.1f}\")\n",
    "\n",
    "# ONNX FP32\n",
    "size = Path('weights/best.onnx').stat().st_size / 1024 / 1024\n",
    "mean, std = benchmark_onnx('weights/best.onnx')\n",
    "print(f\"{'ONNX FP32':<25} {size:<12.1f} {mean:<15.2f} {1000/mean:<10.1f}\")\n",
    "\n",
    "# ONNX INT8\n",
    "if Path('weights/best_int8.onnx').exists():\n",
    "    size = Path('weights/best_int8.onnx').stat().st_size / 1024 / 1024\n",
    "    mean, std = benchmark_onnx('weights/best_int8.onnx')\n",
    "    print(f\"{'ONNX INT8':<25} {size:<12.1f} {mean:<15.2f} {1000/mean:<10.1f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load model\n",
    "model = YOLO('weights/best.pt')\n",
    "\n",
    "# Get sample image\n",
    "sample_images = list(Path('data/processed/images/val').glob('*.jpg'))[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, img_path in zip(axes, sample_images):\n",
    "    results = model(str(img_path), verbose=False)[0]\n",
    "    annotated = results.plot()\n",
    "    ax.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{len(results.boxes)} detections\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_detections.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved sample_detections.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory in Drive\n",
    "!mkdir -p \"/content/drive/MyDrive/adas-detection-results\"\n",
    "\n",
    "# Copy weights\n",
    "!cp -r weights \"/content/drive/MyDrive/adas-detection-results/\"\n",
    "\n",
    "# Copy training results\n",
    "!cp -r runs/train/yolov8m-bdd100k \"/content/drive/MyDrive/adas-detection-results/\"\n",
    "\n",
    "# Copy sample detections\n",
    "!cp sample_detections.png \"/content/drive/MyDrive/adas-detection-results/\"\n",
    "\n",
    "print(\"\\nSaved to Google Drive!\")\n",
    "print(\"Location: My Drive/adas-detection-results/\")\n",
    "!ls -lh \"/content/drive/MyDrive/adas-detection-results/weights/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "Training complete! Your trained models are saved to:\n",
    "- `My Drive/adas-detection-results/weights/best.pt` - PyTorch model\n",
    "- `My Drive/adas-detection-results/weights/best.onnx` - ONNX FP32\n",
    "- `My Drive/adas-detection-results/weights/best_int8.onnx` - ONNX INT8\n",
    "- `My Drive/adas-detection-results/weights/best_fp16.engine` - TensorRT FP16\n",
    "\n",
    "Next steps:\n",
    "1. Download the weights to your local machine\n",
    "2. Place them in `weights/` folder of your project\n",
    "3. Run the Gradio demo or FastAPI server"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
